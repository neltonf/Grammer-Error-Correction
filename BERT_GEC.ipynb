{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_GEC_Implementation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LfiBXakLdXPM"},"source":["# **Grammar Error Correction using BERT**\n","\n","\n","***Use of BERT Masked Language Model (MLM) for Grammar Error Correction (GEC), without the use of annotated data***\n","\n","Sunil Chomal | sunilchomal@gmail.com"]},{"cell_type":"code","metadata":{"id":"PXyJCXB6p_jc","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/GEC.png":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":"Not Found"}},"base_uri":"https://localhost:8080/","height":45},"executionInfo":{"status":"ok","timestamp":1617697500226,"user_tz":-330,"elapsed":3302,"user":{"displayName":"Nelton Fernandes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghmoxx1pEQ50egn57xa0YFlk15HC1klcnUAY6jAR9k=s64","userId":"00557634069417828762"}},"outputId":"9b35abb1-dd72-4371-e7f5-66264a657755"},"source":["%%html\n","<img src='/nbextensions/google.colab/GEC.png' />"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<img src='/nbextensions/google.colab/GEC.png' />"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"2u7iMiMEdNez"},"source":[" **High level workflow**\n"," \n","•\tTokenize the sentence using Spacy\n","\n","•\tCheck for spelling errors using Hunspell\n","\n","•\tFor all preposition, determiners & helper verbs, create a set of probable sentences\n","\n","•\tCreate a set of sentences with each word “masked”, deleted or an additional determiner, preposition or helper verb added\n","\n","•\tUsed BERT Masked Language Model to determine possible suggestions for masks\n","\n","•\tUse the GED model to select appropriate solutions\n"]},{"cell_type":"code","metadata":{"id":"d0be7sVJ8hFj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617697508926,"user_tz":-330,"elapsed":11994,"user":{"displayName":"Nelton Fernandes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghmoxx1pEQ50egn57xa0YFlk15HC1klcnUAY6jAR9k=s64","userId":"00557634069417828762"}},"outputId":"9d21d79a-920a-47c7-d573-f497ac5e61d6"},"source":["# install pytorch_pretrained_bert the previous version of Pytorch-Transformers\n","!pip install -U pytorch_pretrained_bert"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pytorch_pretrained_bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\r\u001b[K     |██▋                             | 10kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 13.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 10.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 10.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 10.2MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n","Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n","Requirement already satisfied, skipping upgrade: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.8.1+cu101)\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/2e/885bea97567ed610fda71ffb4f9ffc02715589277c6f480ef879dcbe9944/boto3-1.17.45-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 16.9MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n","Collecting botocore<1.21.0,>=1.20.45\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d7/efd70bd8a4a4916b8e3b3fe1d5b43bc53d8e43403b1b8beb0747c96e7e75/botocore-1.20.45-py2.py3-none-any.whl (7.4MB)\n","\u001b[K     |████████████████████████████████| 7.4MB 16.3MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n","\u001b[K     |████████████████████████████████| 81kB 9.0MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.45->boto3->pytorch_pretrained_bert) (2.8.1)\n","Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.45->boto3->pytorch_pretrained_bert) (1.15.0)\n","\u001b[31mERROR: botocore 1.20.45 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","Successfully installed boto3-1.17.45 botocore-1.20.45 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"41MNRTbh7qBm"},"source":["import torch\n","from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ne5HvQLD7rCU","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1617697513077,"user_tz":-330,"elapsed":16135,"user":{"displayName":"Nelton Fernandes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghmoxx1pEQ50egn57xa0YFlk15HC1klcnUAY6jAR9k=s64","userId":"00557634069417828762"}},"outputId":"70fbee46-d925-45f1-a87f-d0c8037c55b7"},"source":["# Check to confirm that GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla K80'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"af63QP2z7s7s"},"source":["# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n","import logging\n","logging.basicConfig(level=logging.INFO)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-72xZeds8KNQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617697513843,"user_tz":-330,"elapsed":16895,"user":{"displayName":"Nelton Fernandes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghmoxx1pEQ50egn57xa0YFlk15HC1klcnUAY6jAR9k=s64","userId":"00557634069417828762"}},"outputId":"9c2c9dea-8e87-4edf-a1e5-dfd08f6a53b6"},"source":["# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmp9nnny_41\n","100%|██████████| 231508/231508 [00:00<00:00, 931858.25B/s]\n","INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmp9nnny_41 to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmp9nnny_41\n","INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YCaEFtjS7vSq"},"source":["from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","def check_GE(sents):\n","  #   \"\"\"Check of the input sentences have grammatical errors\n","\n","  #   :param list: list of sentences\n","  #   :return: error, probabilities\n","  #   :rtype: (boolean, (float, float))\n","  #   \"\"\"\n","    \n","  # # Create sentence) and label lists\n","  # # We need to add special tokens at the beginning and end of each sentence\n","  # # for BERT to work properly\n","  sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sents]\n","  labels =[0]\n","\n","  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","  # Padding Sentences\n","  # Set the maximum sequence length. The longest sequence in our training set\n","  # is 47, but we'll leave room on the end anyway.\n","  # In the original paper, the authors used a length of 512.\n","  MAX_LEN = 128\n","\n","  predictions = []\n","  true_labels = []\n","\n","  # Pad our input tokens\n","  input_ids = pad_sequences(\n","      [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], \n","      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\"\n","      )\n","\n","  # Index Numbers and Padding\n","  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","  # pad sentences\n","  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                            dtype =\"long\", truncating=\"post\",padding =\"post\")\n","\n","  # Attention masks\n","  # Create attention masks\n","  attention_masks = []\n","\n","  # Create a mask of 1s for each token followed by 0s for padding\n","  for seq in input_ids:\n","    seq_mask = [float(i > 0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","  prediction_inputs = torch.tensor(input_ids)\n","  prediction_masks = torch.tensor(attention_masks)\n","  prediction_labels = torch.tensor(labels)\n","\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    logits = modelGED(prediction_inputs, token_type_ids=None, \n","                      attention_mask=prediction_masks)\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  # label_ids = b_labels.to(\"cpu\").numpy()\n","\n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  # true_labels.append(label_ids)\n","\n","#   print(predictions)\n","  flat_predictions = [item for sublist in predictions for item in sublist]\n","#   print(flat_predictions)\n","  prob_vals = flat_predictions\n","  flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","  # flat_true_labels = [item for sublist in true_labels for item in sublist]\n","#   print(flat_predictions)\n","  return flat_predictions, prob_vals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eA4nvXBn7Kf_"},"source":["# load previously trained BERT Grammar Error Detection model\n","\n","# from self google drive\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# !cp './drive/My Drive/Colab Notebooks/S89A/bert-based-uncased-GED.pth' .\n","# !cp './drive/u/0/folders/1vmSgFvTXuSq6CYt56eYOWKOnbHX0vLKm/bert-based-uncased-GED.pth' ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ZjdtFfmzC9i"},"source":["#\n","# CREDIT: https://stackoverflow.com/a/39225039\n","#\n","\n","import requests\n","\n","def download_file_from_google_drive(id, destination):\n","  print(\"Trying to fetch {}\".format(destination))\n","\n","  def get_confirm_token(response):\n","    for key, value in response.cookies.items():\n","      if key.startswith('download_warning'):\n","        return value\n","\n","    return None\n","\n","  def save_response_content(response, destination):\n","    CHUNK_SIZE = 32768\n","\n","    with open(destination, \"wb\") as f:\n","      for chunk in progress_bar(response.iter_content(CHUNK_SIZE)):\n","        if chunk: # filter out keep-alive new chunks\n","          f.write(chunk)\n","\n","  URL = \"https://docs.google.com/uc?export=download\"\n","\n","  session = requests.Session()\n","\n","  response = session.get(URL, params = { 'id' : id }, stream = True)\n","  token = get_confirm_token(response)\n","\n","  if token:\n","    params = { 'id' : id, 'confirm' : token }\n","    response = session.get(URL, params = params, stream = True)\n","\n","  save_response_content(response, destination)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YglAn18CzxJW"},"source":["def progress_bar(some_iter):\n","    try:\n","        from tqdm import tqdm\n","        return tqdm(some_iter)\n","    except ModuleNotFoundError:\n","        return some_iter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N72u8k_FzGkB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617697518896,"user_tz":-330,"elapsed":21932,"user":{"displayName":"Nelton Fernandes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghmoxx1pEQ50egn57xa0YFlk15HC1klcnUAY6jAR9k=s64","userId":"00557634069417828762"}},"outputId":"8f6e9cbc-2c21-4e96-d35d-c056727893fb"},"source":["# load previously trained BERT Grammar Error Detection model\n","\n","# download from public google drive link\n","download_file_from_google_drive(\"1al7v87aRxebSUCXrN2Sdd0jGUS0zZ3vn\", \"./bert-based-uncased-GED.pth\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Trying to fetch ./bert-based-uncased-GED.pth\n"],"name":"stdout"},{"output_type":"stream","text":["13367it [00:02, 5640.14it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YdekFl7a7ftt"},"source":["# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n","\n","from pytorch_pretrained_bert import BertForSequenceClassification\n","\n","modelGED = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n","                                                      num_labels=2)\n","\n","# restore model\n","modelGED.load_state_dict(torch.load('bert-based-uncased-GED.pth'))\n","modelGED.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ECUddA_Y9KA3"},"source":["# Load pre-trained model (weights) for Masked Language Model (MLM)\n","model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrrIyrNv-cEF"},"source":["# Load pre-trained model tokenizer (vocabulary)\n","tokenizerLarge = BertTokenizer.from_pretrained('bert-large-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p20iABcF9nYe"},"source":["# install the packages for Hunspell\n","\n","!sudo apt-get install libhunspell-1.6-0 libhunspell-dev\n","!pip install cyhunspell"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rG1ow8ew9n_Y","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1617697665936,"user_tz":-330,"elapsed":168951,"user":{"displayName":"Nelton Fernandes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghmoxx1pEQ50egn57xa0YFlk15HC1klcnUAY6jAR9k=s64","userId":"00557634069417828762"}},"outputId":"91890b46-246d-489d-9e67-701f348f5fff"},"source":["from hunspell import Hunspell\n","import os\n","\n","# download the gn_GB dictionary for hunspell\n","download_file_from_google_drive(\"1jC5BVF9iZ0gmRQNmDcZnhfFdEYv8RNok\", \"./en_GB-large.dic\")\n","download_file_from_google_drive(\"1g8PO8kdw-YmyOY_HxjnJ5FfdJFX4bsPv\", \"./en_GB-large.aff\")\n","\n","gb = Hunspell(\"en_GB-large\", hunspell_data_dir=\".\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:root:Generating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n","INFO:root:Generating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n"],"name":"stderr"},{"output_type":"stream","text":["Trying to fetch ./en_GB-large.dic\n"],"name":"stdout"},{"output_type":"stream","text":["27it [00:00, 1198.36it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Trying to fetch ./en_GB-large.aff\n"],"name":"stdout"},{"output_type":"stream","text":["1it [00:00, 918.19it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"s4WCEE35jJKQ"},"source":["# List of common determiners\n","# det = [\"\", \"the\", \"a\", \"an\"]\n","det = ['the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your', 'his', \n","       'her', 'its', 'our', 'their', 'all', 'both', 'half', 'either', 'neither', \n","       'each', 'every', 'other', 'another', 'such', 'what', 'rather', 'quite']\n","\n","# List of common prepositions\n","prep = [\"about\", \"at\", \"by\", \"for\", \"from\", \"in\", \"of\", \"on\", \"to\", \"with\", \n","        \"into\", \"during\", \"including\", \"until\", \"against\", \"among\", \n","        \"throughout\", \"despite\", \"towards\", \"upon\", \"concerning\"]\n","\n","# List of helping verbs\n","helping_verbs = ['am', 'is', 'are', 'was', 'were', 'being', 'been', 'be', \n","                 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', \n","                 'shall', 'should', 'may', 'might', 'must', 'can', 'could']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrGKWPZYNoOX"},"source":["# test sentences\n","\n","org_text = []\n","org_text.append(\"They drank the pub .\")\n","org_text.append(\"I am looking forway to see you soon .\")\n","org_text.append(\"The cat sat at mat .\")\n","org_text.append(\"Giant otters is an apex predator .\")\n","org_text.append('There is no a doubt, tracking system has brought many benefits in this information age .')\n","org_text.append('The were angr .')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QojckfCt9OAd"},"source":["import spacy\n","import numpy as np\n","\n","def create_spelling_set(org_text):\n","  \"\"\" Create a set of sentences which have possible corrected spellings\n","  \"\"\"\n","  \n","  sent = org_text\n","  sent = sent.lower()\n","  sent = sent.strip().split()\n","\n","\n","  nlp = spacy.load(\"en\")\n","  proc_sent = nlp.tokenizer.tokens_from_list(sent)\n","  nlp.tagger(proc_sent)\n","\n","  sentences = []\n","\n","  for tok in proc_sent:\n","    # check for spelling for alphanumeric\n","    if tok.text.isalpha() and not gb.spell(tok.text):\n","      new_sent = sent[:]\n","      # append new sentences with possible corrections\n","      for sugg in gb.suggest(tok.text):\n","        new_sent[tok.i] = sugg\n","        sentences.append(\" \".join(new_sent))\n","\n","  spelling_sentences = sentences\n","\n","  # retain new sentences which have a \n","  # minimum chance of correctness using BERT GED\n","  new_sentences = []\n","  \n","  for sent in spelling_sentences:\n","    no_error, prob_val = check_GE([sent])\n","    exps = [np.exp(i) for i in prob_val[0]]\n","    sum_of_exps = sum(exps)\n","    softmax = [j/sum_of_exps for j in exps]\n","    if(softmax[1] > 0.6):\n","      new_sentences.append(sent)\n","  \n","  \n","  # if no corrections, append the original sentence\n","  if len(spelling_sentences) == 0:\n","    spelling_sentences.append(\" \".join(sent))\n","\n","  # eliminate dupllicates\n","  [spelling_sentences.append(sent) for sent in new_sentences]\n","  spelling_sentences = list(dict.fromkeys(spelling_sentences))\n","\n","  return spelling_sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C7OdVrA9O7O0"},"source":["def create_grammar_set(spelling_sentences):\n","  \"\"\" create a new set of sentences with deleted determiners, \n","      prepositions & helping verbs\n","      \n","  \"\"\"\n","  \n","  new_sentences = []\n","\n","  for text in spelling_sentences:\n","    sent = text.strip().split()\n","    for i in range(len(sent)):\n","      new_sent = sent[:]\n","      \n","      if new_sent[i] not in list(set(det + prep + helping_verbs)):\n","        continue\n","      \n","      del new_sent[i]\n","      text = \" \".join(new_sent)\n","      \n","      # retain new sentences which have a \n","      # minimum chance of correctness using BERT GED\n","      no_error, prob_val = check_GE([text])\n","      exps = [np.exp(i) for i in prob_val[0]]\n","      sum_of_exps = sum(exps)\n","      softmax = [j/sum_of_exps for j in exps]\n","      if(softmax[1] > 0.6):\n","        new_sentences.append(text)\n","  \n","  # eliminate dupllicates\n","  [spelling_sentences.append(sent) for sent in new_sentences]\n","  spelling_sentences = list(dict.fromkeys(spelling_sentences))\n","  return spelling_sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j34k7n2p9Y5q"},"source":["def create_mask_set(spelling_sentences):\n","  \"\"\"For each input sentence create 2 sentences\n","     (1) [MASK] each word\n","     (2) [MASK] for each space between words\n","  \"\"\"\n","  sentences = []\n","\n","  for sent in spelling_sentences:\n","    sent = sent.strip().split()\n","    for i in range(len(sent)):\n","      # (1) [MASK] each word\n","      new_sent = sent[:]\n","      new_sent[i] = '[MASK]'\n","      text = \" \".join(new_sent)\n","      new_sent = '[CLS] ' + text + ' [SEP]'\n","      sentences.append(new_sent)\n","\n","      # (2) [MASK] for each space between words\n","      new_sent = sent[:]\n","      new_sent.insert(i, '[MASK]')\n","      text = \" \".join(new_sent)\n","      new_sent = '[CLS] ' + text + ' [SEP]'\n","      sentences.append(new_sent)\n","\n","  return sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMC3vhj49ZjD"},"source":["import math\n","from difflib import SequenceMatcher\n","\n","def check_grammar(org_sent, sentences, spelling_sentences):\n","  \"\"\" check grammar for the input sentences\n","  \"\"\"\n","  \n","  n = len(sentences)\n","  \n","  # what is the tokenized value of [MASK]. Usually 103\n","  text = '[MASK]'\n","  tokenized_text = tokenizerLarge.tokenize(text)\n","  mask_token = tokenizerLarge.convert_tokens_to_ids(tokenized_text)[0]\n","\n","  LM_sentences = []\n","  new_sentences = []\n","  i = 0 # current sentence number\n","  l = len(org_sent.strip().split())*2 # l is no of sentencees\n","  mask = False # flag indicating if we are processing space MASK\n","\n","  for sent in sentences:\n","    i += 1\n","    \n","    print(\".\", end=\"\")\n","    if i%50 == 0:\n","      print(\"\")\n","    \n","    # tokenize the text\n","    tokenized_text = tokenizerLarge.tokenize(sent)\n","    indexed_tokens = tokenizerLarge.convert_tokens_to_ids(tokenized_text)\n","\n","    # Create the segments tensors.\n","    segments_ids = [0] * len(tokenized_text)\n","\n","    # Convert inputs to PyTorch tensors\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","    segments_tensors = torch.tensor([segments_ids])\n","\n","    # Predict all tokens\n","    with torch.no_grad():\n","        predictions = model(tokens_tensor, segments_tensors)\n","\n","    # index of the masked token\n","    mask_index = (tokens_tensor == mask_token).nonzero()[0][1].item()\n","    # predicted token\n","    predicted_index = torch.argmax(predictions[0, mask_index]).item()\n","    predicted_token = tokenizerLarge.convert_ids_to_tokens([predicted_index])[0]\n","    \n","    # second best prediction. Can you used to create more options\n","#     second_index = torch.topk(predictions[0, mask_index], 2).indices[1].item()\n","#     second_prediction = tokenizer.convert_ids_to_tokens([second_index])[0]\n","\n","    text = sent.strip().split()\n","    mask_index = text.index('[MASK]')\n","\n","    if not mask:\n","      # case of MASKed words\n","      \n","      mask = True\n","      text[mask_index] = predicted_token\n","      try:\n","        # retrieve original word\n","        org_word = spelling_sentences[i//l].strip().split()[mask_index-1]\n","#         print(\">>> \" + org_word)\n","      except:\n","#         print(spelling_sentences[i%l - 1])\n","#         print(tokenized_text)\n","#         print(\"{0} {1} {2}\".format(i, l, mask_index))\n","        print(\"!\", end=\"\")\n","        continue\n","  #     print(\"{0} - {1}\".format(org_word, predicted_token))\n","      # check if the prediction is an inflection of the original word\n","  #   if org_word.isalpha() and predicted_token not in gb_infl[org_word]:\n","  #     continue\n","      # use SequenceMatcher to see if predicted word is similar to original word\n","      if SequenceMatcher(None, org_word, predicted_token).ratio() < 0.6:\n","        if org_word not in list(set(det + prep + helping_verbs)) or predicted_token not in list(set(det + prep + helping_verbs)):\n","          continue\n","      if org_word == predicted_token:\n","        continue\n","    else:\n","      # case for MASKed spaces\n","      \n","      mask = False\n","  #     print(\"{0}\".format(predicted_token))\n","      # only allow determiners / prepositions  / helping verbs in spaces\n","      if predicted_token in list(set(det + prep + helping_verbs)) :\n","        text[mask_index] = predicted_token\n","      else:\n","        continue\n","\n","  #   if org_word == \"in\":\n","  #     print(\">>>>>> \" + predicted_token)\n","  #   print(tokenized_text)\n","  #   print(mask_index)\n","  \n","    text.remove('[SEP]')\n","    text.remove('[CLS]')\n","    new_sent = \" \".join(text)\n","    \n","  #   print(new_sent)\n","    # retain new sentences which have a \n","    # minimum chance of correctness using BERT GED\n","    no_error, prob_val = check_GE([new_sent])\n","    exps = [np.exp(i) for i in prob_val[0]]\n","    sum_of_exps = sum(exps)\n","    softmax = [j/sum_of_exps for j in exps]\n","    if no_error and softmax[1] > 0.996:\n","  #     print(org_word)\n","  #     print(predicted_token)\n","  #     print(SequenceMatcher(None, org_word, predicted_token).ratio())\n","  #     print(\"{0} - {1}, {2}\".format(prob_val[0][1], prob_val[0][0], prob_val[0][1] - prob_val[0][0]))\n","\n","  #     print(\"{0} - {1:.2f}\".format(new_sent, softmax[1]*100) )\n","      print(\"*\", end=\"\")\n","      new_sentences.append(new_sent)\n","  #   print(\"{0}\\t{1}\".format(predicted_token, second_prediction))\n","\n","  print(\"\")\n","  \n","  # remove duplicate suggestions\n","  spelling_sentences = []\n","  [spelling_sentences.append(sent) for sent in new_sentences]\n","  spelling_sentences = list(dict.fromkeys(spelling_sentences))\n","  spelling_sentences\n","  \n","  return spelling_sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5OizN4gFigm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617697882779,"user_tz":-330,"elapsed":385777,"user":{"displayName":"Nelton Fernandes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghmoxx1pEQ50egn57xa0YFlk15HC1klcnUAY6jAR9k=s64","userId":"00557634069417828762"}},"outputId":"146fdcbb-be7e-465a-81b3-4474b164a6ad"},"source":["# org_text = []\n","# with open(\"./drive/My Drive/Colab Notebooks/S89A/CoNLL_2013_DS.txt\") as file:\n","#   org_text = file.readlines()\n","\n","# predict for each of the test samples\n","\n","for sent in org_text:\n","  \n","  print(\"Input Sentence >>> \" + sent)\n","  \n","  sentences = create_spelling_set(sent)\n","  spelling_sentences = create_grammar_set(sentences)\n","  sentences = create_mask_set(spelling_sentences)\n","  \n","  print(\"processing {0} possibilities\".format(len(sentences)))\n","  \n","  sentences = check_grammar(sent, sentences, spelling_sentences)\n","\n","  print(\"Suggestions & Probabilities\")\n","  \n","  if len(sentences) == 0:\n","    print(\"None\")\n","    continue\n","\n","  no_error, prob_val =  check_GE(sentences)\n","\n","  for i in range(len(prob_val)):\n","    exps = [np.exp(i) for i in prob_val[i]]\n","    sum_of_exps = sum(exps)\n","    softmax = [j/sum_of_exps for j in exps]\n","    print(\"{0} - {1:0.4f}%\".format(sentences[i], softmax[1]*100))\n","  \n","  print(\"-\"*60)\n","  print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input Sentence >>> They drank the pub .\n","processing 10 possibilities\n","......*....\n","Suggestions & Probabilities\n","they drank at the pub . - 99.8071%\n","------------------------------------------------------------\n","\n","Input Sentence >>> I am looking forway to see you soon .\n","processing 126 possibilities\n",".......*..................*..................*.*......\n","..............*.........!........*...................\n","...*......!...*.*.*............\n","Suggestions & Probabilities\n","i am looking forward to see you soon . - 99.7493%\n","i am looking to Norway to see you soon . - 99.7757%\n","i am looking for a way to see you soon . - 99.7564%\n","i am looking forward to seeing you soon . - 99.7722%\n","am i looking forward to see you soon . - 99.6467%\n","i look forward to see you soon . - 99.7621%\n","------------------------------------------------------------\n","\n","Input Sentence >>> The cat sat at mat .\n","processing 12 possibilities\n","..........*..\n","Suggestions & Probabilities\n","the cat sat at the mat . - 99.8284%\n","------------------------------------------------------------\n","\n","Input Sentence >>> Giant otters is an apex predator .\n","processing 26 possibilities\n",".....*...............*......\n","Suggestions & Probabilities\n","giant otters are an apex predator . - 99.8082%\n","------------------------------------------------------------\n","\n","Input Sentence >>> There is no a doubt, tracking system has brought many benefits in this information age .\n","processing 152 possibilities\n","..................................................\n","......................*..*..........................\n","..................................................\n","..\n","Suggestions & Probabilities\n","there is no doubt, the tracking system has brought many benefits in this information age . - 99.6639%\n","there is no doubt, tracking the system has brought many benefits in this information age . - 99.6644%\n","------------------------------------------------------------\n","\n","Input Sentence >>> The were angr .\n","processing 94 possibilities\n",".........*........*................*................!.\n",".......!.....................................\n","Suggestions & Probabilities\n","they were anger . - 99.8171%\n","they were angry . - 99.8404%\n","they were engr . - 99.7457%\n","------------------------------------------------------------\n","\n"],"name":"stdout"}]}]}